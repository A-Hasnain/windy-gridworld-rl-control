# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19eMCX0UZODRZ_IR-wN6Ow8tfd_ls4Slc
"""

import numpy as np

class Bandit:
    def __init__(self, k=10):
        self.k = k
        self.q_true = np.random.normal(0, 1, k)
        self.best_action = np.argmax(self.q_true)

    def pull(self, action):
        return np.random.normal(self.q_true[action], 1.0)

def gradient_bandit(bandit, steps=1000, alpha=0.1, use_baseline=False):
    k = bandit.k
    H = np.zeros(k)              # Preferences
    pi = np.ones(k) / k          # Initial policy uniform
    rewards = []
    average_reward = 0
    chosen_best = 0

    for step in range(steps):
        pi = np.exp(H) / np.sum(np.exp(H))
        action = np.random.choice(np.arange(k), p=pi)
        reward = bandit.pull(action)
        rewards.append(reward)
        if action == bandit.best_action:
            chosen_best += 1

        if use_baseline:
            baseline = average_reward
        else:
            baseline = 0

        # Update preferences
        H[action] += alpha * (reward - baseline) * (1 - pi[action])
        for a in range(k):
            if a != action:
                H[a] -= alpha * (reward - baseline) * pi[a]
        average_reward += (reward - average_reward) / (step + 1)

    return rewards, chosen_best / steps

import matplotlib.pyplot as plt

def run_experiment(settings, runs=2000, steps=1000):
    results = {}
    for setting in settings:
        label = f"{'baseline' if setting['baseline'] else 'no baseline'}, alpha={setting['alpha']}"
        all_rewards = []
        best_action_ratios = []
        for run in range(runs):
            bandit = Bandit()
            rewards, best_action_ratio = gradient_bandit(
                bandit, steps=steps, alpha=setting['alpha'],
                use_baseline=setting['baseline'])
            all_rewards.append(np.array(rewards))
            best_action_ratios.append(best_action_ratio)
        avg_rewards = np.mean(all_rewards, axis=0)
        avg_best_action_ratio = np.mean(best_action_ratios)
        results[label] = (avg_rewards, avg_best_action_ratio)
    return results

settings = [
    {"baseline": False, "alpha": 0.1},
    {"baseline": False, "alpha": 0.4},
    {"baseline": True, "alpha": 0.1},
    {"baseline": True, "alpha": 0.4},
]

results = run_experiment(settings)

plt.figure(figsize=(10,6))
for label in results:
    plt.plot(results[label][0], label=label)
plt.xlabel("Steps")
plt.ylabel("Average Reward")
plt.title("Gradient Bandit Results (Reproduction of Figure 2.2)")
plt.legend()
plt.savefig("figure22.png")  # Save the figure for submission
plt.show()

"""This submission implements a multi-armed bandit reinforcement learning framework with support for gradient policy updates. The code produces average reward curves for four settings: with and without a baseline and two different learning rates, as required by the assignment. The experiment reproduces Figure 2.2 from Sutton & Barto’s RL textbook. The provided code and figure demonstrate the algorithm’s performance across specified configurations

"""