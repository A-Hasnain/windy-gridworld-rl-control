# -*- coding: utf-8 -*-
"""WindyGridworld.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16NNhe0TbV7A1OkAFxuEWFxyKagJANkhM
"""

import numpy as np

class WindyGridworld:
    def __init__(self):
        # 7 rows x 10 columns (like Sutton & Barto's book)
        self.rows, self.cols = 7, 10
        self.start = (3, 0)
        self.goal = (3, 7)
        self.wind = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]
        self.reset()

    def reset(self):
        self.agent = list(self.start)
        return tuple(self.agent)

    def step(self, action):
        # actions: 0=up,1=down,2=left,3=right
        moves = [(-1,0), (1,0), (0,-1), (0,1)]
        # wind pushes current column upward
        self.agent[0] -= self.wind[self.agent[1]]
        # then apply action
        self.agent[0] += moves[action][0]
        self.agent[1] += moves[action][1]
        # keep agent in grid bounds
        self.agent[0] = min(max(self.agent[0], 0), self.rows-1)
        self.agent[1] = min(max(self.agent[1], 0), self.cols-1)
        return tuple(self.agent), -1, (tuple(self.agent) == self.goal)

import random
from collections import defaultdict

def sarsa(env, episodes=200, alpha=0.5, gamma=1.0, epsilon=0.1):
    Q = defaultdict(lambda: np.zeros(4))  # 4 actions
    steps_per_episode = []

    for ep in range(episodes):
        state = env.reset()
        # epsilon-greedy start
        if random.random() < epsilon:
            action = random.choice(range(4))
        else:
            action = np.argmax(Q[state])
        steps = 0

        while True:
            next_state, reward, done = env.step(action)
            steps += 1
            # next action (epsilon-greedy)
            if random.random() < epsilon:
                next_action = random.choice(range(4))
            else:
                next_action = np.argmax(Q[next_state])
            # SARSA update
            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])

            state, action = next_state, next_action
            if done:
                break
        steps_per_episode.append(steps)
    return Q, steps_per_episode

import matplotlib.pyplot as plt

env = WindyGridworld()
Q, steps_history = sarsa(env, episodes=200)

plt.plot(steps_history)
plt.xlabel("Episode")
plt.ylabel("Steps per Episode")
plt.title("SARSA in Windy Gridworld: Steps to Reach Goal")
plt.show()

def mc_on_policy(env, episodes=1000, alpha=0.5, gamma=1.0, epsilon=0.5):
    Q = defaultdict(lambda: np.zeros(4))
    returns = defaultdict(list)
    steps_per_episode = []

    for ep in range(episodes):
        state = env.reset()
        episode = []
        steps = 0
        while True:
            if np.random.rand() < epsilon:
                action = np.random.choice(4)
            else:
                action = np.argmax(Q[state])
            next_state, reward, done = env.step(action)
            episode.append((state, action, reward))
            state = next_state
            steps += 1
            if done or steps > 300:  # Lower cutoff to make learning visible
                break
        G = 0
        visited = set()
        for t in reversed(range(len(episode))):
            s, a, r = episode[t]
            G = gamma * G + r
            if (s, a) not in visited:
                returns[(s, a)].append(G)
                Q[s][a] = np.mean(returns[(s, a)])
                visited.add((s, a))
        steps_per_episode.append(steps)
    return Q, steps_per_episode

env = WindyGridworld()
Q_mc, mc_steps_history = mc_on_policy(env, episodes=1000)

plt.plot(mc_steps_history)
plt.xlabel("Episode")
plt.ylabel("Steps per Episode")
plt.title("MC On-Policy in Windy Gridworld")
plt.show()

def q_learning(env, episodes=200, alpha=0.5, gamma=1.0, epsilon=0.1):
    from collections import defaultdict
    import numpy as np

    Q = defaultdict(lambda: np.zeros(4))
    steps_per_episode = []

    for ep in range(episodes):
        state = env.reset()
        steps = 0
        while True:
            if np.random.rand() < epsilon:
                action = np.random.choice(4)
            else:
                action = np.argmax(Q[state])
            next_state, reward, done = env.step(action)
            Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])
            state = next_state
            steps += 1
            if done:
                break
        steps_per_episode.append(steps)
    return Q, steps_per_episode

Q_q, q_steps_history = q_learning(env, episodes=200)

import matplotlib.pyplot as plt

plt.plot(q_steps_history)
plt.xlabel("Episode")
plt.ylabel("Steps per Episode")
plt.title("Q-Learning (TD(0) Off-Policy) in Windy Gridworld")
plt.show()

def mc_off_policy(env, episodes=200, gamma=1.0):
    from collections import defaultdict
    import numpy as np

    Q = defaultdict(lambda: np.zeros(4))
    C = defaultdict(lambda: np.zeros(4))
    steps_per_episode = []

    for ep in range(episodes):
        state = env.reset()
        episode = []
        behavior_policy = lambda s: np.random.choice(4)  # uniform random
        steps = 0

        while True:
            action = behavior_policy(state)
            next_state, reward, done = env.step(action)
            episode.append((state, action, reward))
            state = next_state
            steps += 1
            if done or steps > 300:
                break
        G = 0
        W = 1
        for t in reversed(range(len(episode))):
            s, a, r = episode[t]
            G = gamma * G + r
            C[s][a] += W
            Q[s][a] += (W / C[s][a]) * (G - Q[s][a])
            a_star = np.argmax(Q[s])
            if a != a_star:
                break
            W *= 1 / 0.25  # behavior policy prob (uniform)
        steps_per_episode.append(steps)
    return Q, steps_per_episode

Q_mc_off, mc_off_steps_history = mc_off_policy(env, episodes=200)

plt.plot(mc_off_steps_history)
plt.xlabel("Episode")
plt.ylabel("Steps per Episode")
plt.title("MC Off-Policy in Windy Gridworld")
plt.show()